[{"content":"Hello! I\u0026rsquo;m Jason, a Data Analyst skilled in distilling actionable insights from intricate datasets. Proficient in statistical methodologies and programming languages like Python and R, alongside data visualization tools such as Tableau, I excel in crafting compelling narratives from raw data. Explore my portfolio to witness how I leverage data-driven methodologies to empower informed decision-making.\nAs a fast learner, I readily absorb new information and adapt to new technology. I exhibit strong adaptability and resilience, enabling me to thrive in fast-paced, ever-evolving environments. I embrace unconventional thinking, fostering creativity and innovation as a change maker. I continually push the boundaries of what\u0026rsquo;s achievable, consistently generating fresh ideas. I excel in building rapport and establishing productive relationships with both colleagues and clients. I am committed to ongoing learning and eagerly embrace new challenges. I think outside the box and break the mold to explore innovative solutions. Skills Tech enthusiast: From managing a homelab with 10RU Bare-Metal setups to orchestrating 12 nodes in the Cloud using Docker Swarm. Proficient in a variety of software including Ruby, Python, ReactJS, SQL, R, and Swift. Photographer \u0026amp; ExCofounder of Narval Films Experience Bringing over eight years of dedicated account management experience, I am a proactive professional poised for transition. Recently certified with the Google Data Analytics Professional Certificate, I am now embarking on a journey to pivot into the dynamic realm of data analytics. Eager to leverage my skills in Python, R, SQL, and more, I am actively seeking a full-time role in this burgeoning field.\n1Thing.org Marketing Researcher Dec 2021 - Present, Remote\nBy utilizing my SEO expertise and leveraging tools like Google Analytics. I can significantly increase exposure and drive more traffic to 1Thing.org. Profuse Solution Account Manager, Dec 2013 - Present, Los Angeles\nBuild and maintain positive relationships with C-level clients by effectively resolving their problems in Psychz Network. Using script language, assist in the completion of order processing automation and sales data analytic. Successfully attained sales record significantly. Contact If you\u0026rsquo;re interested in working together, please feel free to contact me:\nKeep in touch\n","permalink":"https://httplife.com/about/","summary":"Hello! I\u0026rsquo;m Jason, a Data Analyst skilled in distilling actionable insights from intricate datasets. Proficient in statistical methodologies and programming languages like Python and R, alongside data visualization tools such as Tableau, I excel in crafting compelling narratives from raw data. Explore my portfolio to witness how I leverage data-driven methodologies to empower informed decision-making.\nAs a fast learner, I readily absorb new information and adapt to new technology. I exhibit strong adaptability and resilience, enabling me to thrive in fast-paced, ever-evolving environments.","title":"About"},{"content":" If you\u0026rsquo;re interested in working together, please feel free to contact me:\nEmail ","permalink":"https://httplife.com/contact/","summary":" If you\u0026rsquo;re interested in working together, please feel free to contact me:\nEmail ","title":"Contact"},{"content":"Objective The purpose is to provide analysis based on dataset1 provided by going through all the necessary step and methods in this alongside explanation. My project collects data of sales records. I have wrangled and analysed data from last couple years of each country individually and visualized the data in form of excel dashboard for easy understanding of lending trend.\nImport \u0026amp;\u0026amp; Read Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import os import pandas as pd import matplotlib.pyplot as plt #find file path directory from dirname, _, filenames in os.walk(\u0026#34;./source/data\u0026#34;): for filename in filenames: print(os.path.join(dirname, filename)) all_sales_df = pd.DataFrame() for filename in os.listdir(dir_path): df = pd.read_csv(os.path.join(dirname, filename)) all_sales_df = pd.concat([all_sales_df, df], ignore_index=True) all_sales_df = all_sales_df.rename(mapper = str.strip, axis =\u0026#39;columns\u0026#39;) all_sales_df = all_sales_df.rename(columns= {\u0026#39;Order ID\u0026#39;: \u0026#39;Order_id\u0026#39;, \u0026#39;Quantity Ordered\u0026#39;: \u0026#39;Quantity\u0026#39;, \u0026#39;Price Each\u0026#39;: \u0026#39;Price\u0026#39;, \u0026#39;Order Date\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Purchase Address\u0026#39;: \u0026#39;Address\u0026#39;}) #lowercase column name column_name = list(all_sales_df.columns) column_name = [x.lower().strip() for x in column_name] all_sales_df.columns = column_name Data cleaning First check for rows contain null and duplicates. Then check for non-numeric values in order_id, quantity, and price since the 3 column cannot be non-numeric. Then, correct format for each column, Int, Timestamp and String. When there is a few consideration about dropping duplicated rows. Check duplicated rows and their duplicates.\nIf there is null and duplicated rows then it need to be dropped/removed. For non-numeric in mentioned columns, apply to_numeric to remove the rows dataframe. Then check the result by applying the previous methods for a new defined dataframe(clean_sales_df).\nidentify data, null and non-numeric digits 1 2 3 4 5 6 7 8 9 #check for all rows contain null value all_sales_df.isnull().sum() #check duplicated rows all_sales_df.duplicated().sum() #check for non-numeric in order_id, quantity, and price for i in [\u0026#34;order_id\u0026#34;, \u0026#34;quantity\u0026#34;, \u0026#34;price\u0026#34;]: all_sales_df[i].loc[pd.to_numeric(all_sales_df[i], errors=\u0026#39;coerce\u0026#39;).isnull()].unique() #check order id in duplicated rows that are not null and is numeric all_sales_df[all_sales_df.duplicated(keep=False) \u0026amp; all_sales_df[\u0026#39;order_id\u0026#39;].notnull() \u0026amp; all_sales_df[\u0026#39;order_id\u0026#39;].str.isnumeric()][\u0026#39;order_id\u0026#39;].head() remove null rows 1 2 3 4 5 #drop null rows clean_sales_df = all_sales_df.dropna(how=\u0026#39;all\u0026#39;) for i in [\u0026#34;order_id\u0026#34;, \u0026#34;quantity\u0026#34;, \u0026#34;price\u0026#34;]: clean_sales_df = clean_sales_df[pd.to_numeric(clean_sales_df[i], errors=\u0026#39;coerce\u0026#39;).notnull()] chagne data type 1 2 3 4 5 6 7 8 9 10 11 12 #change data type for column quantity, price, date, and address clean_sales_df[\u0026#39;quantity\u0026#39;] = clean_sales_df[\u0026#39;quantity\u0026#39;].astype(int) clean_sales_df[\u0026#39;price\u0026#39;] = clean_sales_df[\u0026#39;price\u0026#39;].astype(float) clean_sales_df[\u0026#39;date\u0026#39;] = pd.to_datetime(clean_sales_df[\u0026#39;date\u0026#39;], format=\u0026#39;%m/%d/%y %H:%M\u0026#39;) clean_sales_df[\u0026#39;address\u0026#39;] = clean_sales_df[\u0026#39;address\u0026#39;].astype(str) #add city and state column clean_sales_df[\u0026#39;city\u0026#39;] = clean_sales_df[\u0026#39;address\u0026#39;].apply(lambda x: x.split(\u0026#39;,\u0026#39;)[1].strip()) clean_sales_df[\u0026#39;state\u0026#39;] = clean_sales_df[\u0026#39;address\u0026#39;].apply(lambda x: x.split(\u0026#39;,\u0026#39;)[2].split(\u0026#39; \u0026#39;)[1].strip()) #add total sales column clean_sales_df[\u0026#39;total_sales\u0026#39;] = clean_sales_df[\u0026#39;quantity\u0026#39;] * clean_sales_df[\u0026#39;price\u0026#39;] ** store cleaned dataset to new location **\nQuestions Task 1 Q: What was the best Year for sales? How much was earned that Year? A: Create Annual Sales summary and find max/min values in Annual Sales\nIn this section clean_sales_df will be referred to find annual sales as annual_sales_df. Plot the annual_sales_df and find max/min values in annual_sales_df then print the result as conclusion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #find annual sales summary and convert to dataframe annual_sales_df = pd.DataFrame(clean_sales_df.groupby(clean_sales_df[\u0026#39;date\u0026#39;].dt.year)[\u0026#39;total_sales\u0026#39;].sum()) annual_sales_df.reset_index(inplace=True) #generate annual sales visual with plt.style.context(\u0026#39;ggplot\u0026#39;): plt.figure(figsize=(4, 4)) sns.barplot(x=\u0026#39;date\u0026#39;, y=\u0026#39;total_sales\u0026#39;, data=annual_sales_df) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Annual Sales\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Year\u0026#34;) plt.ylabel(\u0026#34;Sales\u0026#34;) #add data callouts for x, y in enumerate(annual_sales_df[\u0026#39;total_sales\u0026#39;]): label = f\u0026#34;${y:,.2f}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#39;offset points\u0026#39;, xytext=(0,1), ha=\u0026#39;center\u0026#39;, fontsize=12) plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #find highest year and value in annual sales highest_year = annual_sales_df[\u0026#39;date\u0026#39;].iloc[annual_sales_df[\u0026#39;total_sales\u0026#39;].idxmax()] highest_value = annual_sales_df[\u0026#39;total_sales\u0026#39;].max() #find lowest year and value in annual sales lowest_year = annual_sales_df[\u0026#39;date\u0026#39;].iloc[annual_sales_df[\u0026#39;total_sales\u0026#39;].idxmin()] lowest_value = annual_sales_df[\u0026#39;total_sales\u0026#39;].min() #show result as visual with plt.style.context(\u0026#39;ggplot\u0026#39;): plt.subplots(figsize=(8, 1)) plt.axis(\u0026#39;off\u0026#39;) plt.text(0.5, 0.5, f\u0026#34;Highest Year: {highest_year}, Total Sales: ${highest_value:,.2f}\\n\u0026#34; f\u0026#34;Lowest Year: {lowest_year}, Total Sales: ${lowest_value:,.2f}\u0026#34;, fontsize=14, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) plt.title(\u0026#34;Best \u0026amp; Worst Year\u0026#34;, fontsize=20) plt.show() 1 2 3 4 5 6 #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Total sales generated by company: ${highest_value+lowest_value:,.2f}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;With the highest sales is in {highest_year}, generating: ${highest_value:,.2f}\\n\u0026#34; f\u0026#34;With the lowest sales is in {lowest_year}, generating: ${lowest_value:,.2f}\u0026#34;) Task 2 Q: What was the best month for sales? How much was earned that month? A: Create Monthly Sales summary and find largest/smallest values in Monthly Sales\nIn this section clean_sales_df will be referred to find monthly sales as monthly_sales_df. Plot the monthly_sales_df and find largest/smallest values in monthly_sales_df then print the result as conclusion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #find monthly sales summary and convert to dataframe monthly_sales_df = pd.DataFrame(clean_sales_df.groupby([clean_sales_df[\u0026#39;date\u0026#39;].dt.year, clean_sales_df[\u0026#39;date\u0026#39;].dt.month])[\u0026#39;total_sales\u0026#39;].sum()) monthly_sales_df.index = pd.to_datetime(monthly_sales_df.index.map(\u0026#39;{0[0]}-{0[1]}\u0026#39;.format)) monthly_sales_df.index.name = \u0026#39;date\u0026#39; monthly_sales_df = monthly_sales_df.reset_index() #find monthly sales average average_monthly_sales = monthly_sales_df[\u0026#39;total_sales\u0026#39;].mean() #generate monthly sales visual with plt.style.context(\u0026#39;ggplot\u0026#39;): plt.figure(figsize=(16, 4)) sns.lineplot(x=\u0026#39;date\u0026#39;, y=\u0026#39;total_sales\u0026#39;, data=monthly_sales_df) plt.gca().xaxis.set_major_formatter(DateFormatter(\u0026#39;%y-%m\u0026#39;)) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Monthly Sales\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Date\u0026#34;) plt.ylabel(\u0026#34;Sales\u0026#34;) plt.xticks(monthly_sales_df[\u0026#39;date\u0026#39;]) #add axh line plt.axhline(y=average_monthly_sales, color=\u0026#39;blue\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=1) plt.text(monthly_sales_df[\u0026#39;date\u0026#39;].iloc[-12], average_monthly_sales+5, f\u0026#34;Avg: ${average_monthly_sales:,.2f}\u0026#34;, color=\u0026#39;blue\u0026#39;, fontsize=10, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) #add data callouts for x, y in zip(monthly_sales_df[\u0026#39;date\u0026#39;], monthly_sales_df[\u0026#39;total_sales\u0026#39;]): label = f\u0026#34;${y:,.2f}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#39;offset points\u0026#39;, xytext=(2,4), ha=\u0026#39;center\u0026#39;) plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #find highest month and value in monthly sales highest_months = monthly_sales_df.nlargest(3, \u0026#39;total_sales\u0026#39;) highest_months[\u0026#39;date\u0026#39;] = pd.to_datetime(highest_months[\u0026#39;date\u0026#39;]).dt.strftime(\u0026#39;%B %Y\u0026#39;) highest_month_names = highest_months[\u0026#39;date\u0026#39;].tolist() highest_month_values = highest_months[\u0026#39;total_sales\u0026#39;].tolist() #find lowest month and value in monthly sales lowest_months = monthly_sales_df.nsmallest(3, \u0026#39;total_sales\u0026#39;) lowest_months[\u0026#39;date\u0026#39;] = pd.to_datetime(lowest_months[\u0026#39;date\u0026#39;]).dt.strftime(\u0026#39;%B %Y\u0026#39;) lowest_month_names = lowest_months[\u0026#39;date\u0026#39;].tolist() lowest_month_values = lowest_months[\u0026#39;total_sales\u0026#39;].tolist() #show result as visual with plt.style.context(\u0026#39;ggplot\u0026#39;): plt.subplots(figsize=(8, 1)) plt.axis(\u0026#39;off\u0026#39;) plt.text(0.5, 0.5, f\u0026#34;Highest Month: {highest_month_names[0]}, Total Sales: ${highest_month_values[0]:,.2f}\\n\u0026#34; f\u0026#34;Lowest Month: {lowest_month_names[0]}, Total Sales: ${lowest_month_values[0]:,.2f}\u0026#34;, fontsize=14, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) plt.title(\u0026#34;Best \u0026amp; Worst Month\u0026#34;, fontsize=20) plt.show() 1 2 3 4 5 #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Top 3 best month:\\n{highest_months.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Bottom 3 worst month:\\n{lowest_months.to_string(index=False)}\u0026#34;) Task 3 Q: What City had the highest number of sales? A: Create Sales by State/City and find largest/smallest values in Sales by State/City\nThis section separated by 2 for Sales by State and Sales by City from clean_sales_df. Purpose is to find both trend on state level and city level. Plot and find largest/smallest values in both dataframe then print the result as conclusion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #find sales by state and convert to dataframe sales_by_state_df = pd.DataFrame(clean_sales_df.groupby(\u0026#39;state\u0026#39;)[\u0026#39;total_sales\u0026#39;].sum()) sales_by_state_df = sales_by_state_df.sort_values(by=\u0026#39;total_sales\u0026#39;, ascending=False).reset_index() #generate sales by state visual with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): plt.figure(figsize=(16, 4)) sns.barplot(x=\u0026#39;state\u0026#39;, y=\u0026#39;total_sales\u0026#39;, data=sales_by_state_df ,palette=\u0026#39;husl\u0026#39;) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Sales by State\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;State\u0026#34;) plt.ylabel(\u0026#34;Sales\u0026#34;) #add data callouts for x, y in enumerate(sales_by_state_df[\u0026#39;total_sales\u0026#39;]): label = f\u0026#34;${y:,.2f}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#34;offset points\u0026#34;, xytext=(0,2), ha=\u0026#39;center\u0026#39;, fontsize=10) plt.show() 1 2 3 4 5 6 7 8 9 #create dictionary for state colors for use in sales by city visual state_colors = {} n_states = len(sales_by_state_df[\u0026#39;state\u0026#39;]) palette = sns.color_palette(\u0026#39;husl\u0026#39;, n_colors=n_states) #use same palette as previous state visual for i, state in enumerate(sales_by_state_df[\u0026#39;state\u0026#39;]): state_colors[state] = palette[i] #check the state colors print(state_colors) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #find highest month and value in sales by state highest_state = sales_by_state_df.nlargest(3, \u0026#39;total_sales\u0026#39;) highest_state_names = highest_state[\u0026#39;state\u0026#39;].tolist() highest_state_values = highest_state[\u0026#39;total_sales\u0026#39;].tolist() #find lowest month and value in sales by state lowest_state = sales_by_state_df.nsmallest(3, \u0026#39;total_sales\u0026#39;) lowest_state_names = lowest_state[\u0026#39;state\u0026#39;].tolist() lowest_state_values = lowest_state[\u0026#39;total_sales\u0026#39;].to_list() #show result as visual with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): plt.subplots(figsize=(8, 1)) plt.axis(\u0026#39;off\u0026#39;) plt.text(0.5, 0.5, f\u0026#34;Highest State: {highest_state_names[0]}, Total Sales: ${highest_state_values[0]:,.2f}\\n\u0026#34; f\u0026#34;Lowest State: {lowest_state_names[0]}, Total Sales: ${lowest_state_values[0]:,.2f}\u0026#34;, fontsize=14, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) plt.title(\u0026#34;Best \u0026amp; Worst State\u0026#34;, fontsize=20) plt.show() 1 2 3 4 5 6 7 #find sales by city and convert to dataframe sales_by_city_df = pd.DataFrame(clean_sales_df.groupby([\u0026#39;state\u0026#39;, \u0026#39;city\u0026#39;])[\u0026#39;total_sales\u0026#39;].sum()) sales_by_city_df = sales_by_city_df.sort_values(by=\u0026#39;total_sales\u0026#39;, ascending=False).reset_index() #modify city named \u0026#34;Portland\u0026#34; before generating visual (to avoid overwrite) sales_by_city_df.loc[(sales_by_city_df[\u0026#39;state\u0026#39;] == \u0026#39;ME\u0026#39;) \u0026amp; (sales_by_city_df[\u0026#39;city\u0026#39;] == \u0026#39;Portland\u0026#39;), \u0026#39;city\u0026#39;] = \u0026#39;Portland (Maine)\u0026#39; sales_by_city_df.loc[(sales_by_city_df[\u0026#39;state\u0026#39;] == \u0026#39;OR\u0026#39;) \u0026amp; (sales_by_city_df[\u0026#39;city\u0026#39;] == \u0026#39;Portland\u0026#39;), \u0026#39;city\u0026#39;] = \u0026#39;Portland (Oregon)\u0026#39; 1 2 3 4 5 #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Top 3 best state sales:\\n{highest_state.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Bottom 3 worst state sales:\\n{lowest_state.to_string(index=False)}\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #generate visual for sales by city with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): plt.figure(figsize=(16, 4)) sns.barplot(x=\u0026#39;total_sales\u0026#39;, y=\u0026#39;city\u0026#39;, hue=\u0026#39;state\u0026#39;, data=sales_by_city_df, dodge=False, palette=state_colors) #previous state_colors dict plt.title(\u0026#34;Sales by City\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Sales\u0026#34;) plt.ylabel(\u0026#34;City\u0026#34;) plt.legend(title=\u0026#34;State\u0026#34;, bbox_to_anchor=(1.05, 0.5), loc=\u0026#39;center left\u0026#39;, borderaxespad=0.) #add data callouts for y, x in enumerate(sales_by_city_df[\u0026#39;total_sales\u0026#39;]): label = f\u0026#34;${x:,.2f}\u0026#34; plt.annotate(label, (x, y), textcoords=\u0026#39;offset points\u0026#39;, xytext=(0, 0), ha=\u0026#39;left\u0026#39;, va=\u0026#39;center\u0026#39;, fontsize=10) plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #find highest month and value in sales by city highest_city = sales_by_city_df.nlargest(3, \u0026#39;total_sales\u0026#39;) highest_city_names = highest_city[\u0026#39;city\u0026#39;].tolist() highest_city_values = highest_city[\u0026#39;total_sales\u0026#39;].tolist() #find lowest month and value in sales by city lowest_city = sales_by_city_df.nsmallest(3, \u0026#39;total_sales\u0026#39;) lowest_city_names = lowest_city[\u0026#39;city\u0026#39;].tolist() lowest_city_values = lowest_city[\u0026#39;total_sales\u0026#39;].tolist() #show result as visual with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): plt.subplots(figsize=(8, 1)) plt.axis(\u0026#39;off\u0026#39;) plt.text(0.5, 0.5, f\u0026#34;Highest City: {highest_city_names[0]}, Total Sales: ${highest_city_values[0]:,.2f}\\n\u0026#34; f\u0026#34;Lowest City: {lowest_city_names[0]}, Total Sales: ${lowest_city_values[0]:,.2f}\u0026#34;, fontsize=14, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) plt.title(\u0026#34;Highest \u0026amp; Lowest City Sales\u0026#34;, fontsize=20) plt.show() 1 2 3 4 5 #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Top 3 best city sales:\\n{highest_city.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Bottom 3 worst city sales:\\n{lowest_city.to_string(index=False)}\u0026#34;) Task 4 Q: What time should we display adverstisement to maximize likelihood of customer\u0026rsquo;s buying product? A: Find Monthly/Daily/Hourly Order Trend\nThis section will find the data related to orders by simply find count/unique number of order_id by month/day/hour. The data that will be referred here is clean_sales_2019_df(which also apply for the rest of the analysis).\nDespite order id cannot be duplicate in the dataset there is multiple rows with the same order id. The reason why it\u0026rsquo;s not dropped is because each may contains different product. *not sure how to approach/may also need to change data cleaning section\nBut using .count() it will return the duplicated order id as well. If we want to count the order id as unique number then we can call .nunique() which will be used in following sections.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #find monthly order and convert to dataframe monthly_order_df = pd.DataFrame(clean_sales_2019_df.groupby([clean_sales_2019_df[\u0026#39;date\u0026#39;].dt.year, clean_sales_2019_df[\u0026#39;date\u0026#39;].dt.month])[\u0026#39;order_id\u0026#39;].nunique()) monthly_order_df.index = pd.to_datetime(monthly_order_df.index.map(\u0026#39;{0[0]}-{0[1]}\u0026#39;.format)) monthly_order_df.index.name = \u0026#39;date\u0026#39; monthly_order_df = monthly_order_df.rename(columns={\u0026#39;order_id\u0026#39;: \u0026#39;total_orders\u0026#39;}).reset_index() #find average monthly order average_monthly_order = monthly_order_df[\u0026#39;total_orders\u0026#39;].mean() #generate monthly order visual with plt.style.context(\u0026#39;bmh\u0026#39;): plt.figure(figsize=(16, 4)) sns.lineplot(x=\u0026#39;date\u0026#39;, y=\u0026#39;total_orders\u0026#39;, data=monthly_order_df) plt.gca().xaxis.set_major_formatter(DateFormatter(\u0026#39;%Y-%m\u0026#39;)) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Monthly Order Trend\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Date\u0026#34;) plt.ylabel(\u0026#34;Order\u0026#34;) plt.xticks(monthly_order_df[\u0026#39;date\u0026#39;]) #add axh line plt.axhline(y=average_monthly_order, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=1) plt.text(monthly_order_df[\u0026#39;date\u0026#39;].iloc[-12], average_monthly_order+5, f\u0026#34;Avg: {average_monthly_order:,.2f}\u0026#34;, color=\u0026#39;red\u0026#39;, fontsize=10, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) #add data callouts for x, y in zip(monthly_order_df[\u0026#39;date\u0026#39;], monthly_order_df[\u0026#39;total_orders\u0026#39;]): label = f\u0026#34;{(y):,}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#39;offset points\u0026#39;, xytext=(2,4), ha=\u0026#39;center\u0026#39;) plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #find 3 highest value in monthly order highest_monthly_orders = monthly_order_df.nlargest(3, \u0026#39;total_orders\u0026#39;) highest_monthly_orders[\u0026#39;date\u0026#39;] = pd.to_datetime(highest_monthly_orders[\u0026#39;date\u0026#39;]).dt.strftime(\u0026#39;%B\u0026#39;) #find 3 lowest value in monthly order lowest_monthly_orders = monthly_order_df.nsmallest(3, \u0026#39;total_orders\u0026#39;) lowest_monthly_orders[\u0026#39;date\u0026#39;] = pd.to_datetime(lowest_monthly_orders[\u0026#39;date\u0026#39;]).dt.strftime(\u0026#39;%B\u0026#39;) #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Months with highest total orders:\\n{highest_monthly_orders.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Months with lowest total orders:\\n{lowest_monthly_orders.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;*use as references to maximize/prioritize advertisement\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #find daily order and convert to dataframe daily_order_df = pd.DataFrame(clean_sales_2019_df.groupby(clean_sales_2019_df[\u0026#39;date\u0026#39;].dt.dayofweek)[\u0026#39;order_id\u0026#39;].nunique()) daily_order_df.columns = [\u0026#39;total_orders\u0026#39;] daily_order_df.index = [\u0026#39;Monday\u0026#39;, \u0026#39;Tuesday\u0026#39;, \u0026#39;Wednesday\u0026#39;, \u0026#39;Thursday\u0026#39;, \u0026#39;Friday\u0026#39;, \u0026#39;Saturday\u0026#39;, \u0026#39;Sunday\u0026#39;] daily_order_df.index.name = \u0026#39;day_of_week\u0026#39; daily_order_df = daily_order_df.reset_index() #find average daily order average_daily_order = daily_order_df[\u0026#39;total_orders\u0026#39;].mean() #generate daily order visual with plt.style.context(\u0026#39;bmh\u0026#39;): plt.figure(figsize=(16, 4)) sns.lineplot(x=\u0026#39;day_of_week\u0026#39;, y=\u0026#39;total_orders\u0026#39;, data=daily_order_df) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Daily Order Trend\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Day of Week\u0026#34;) plt.ylabel(\u0026#34;Order\u0026#34;) #add axh line plt.axhline(y=average_daily_order, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=1) plt.text(daily_order_df[\u0026#39;day_of_week\u0026#39;].iloc[-7], average_daily_order-5, f\u0026#34;Avg: {average_daily_order:,.2f}\u0026#34;, color=\u0026#39;red\u0026#39;, fontsize=10, ha=\u0026#39;center\u0026#39;, va=\u0026#39;top\u0026#39;) #add data callouts for x, y in enumerate(daily_order_df[\u0026#39;total_orders\u0026#39;]): label = f\u0026#34;{(y):,}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#39;offset points\u0026#39;, xytext=(2,4), ha=\u0026#39;center\u0026#39;) plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 #find 3 highest value in daily orders highest_daily_orders = daily_order_df.nlargest(3, \u0026#39;total_orders\u0026#39;) #find 3 lowest value in daily orders lowest_daily_orders = daily_order_df.nsmallest(3, \u0026#39;total_orders\u0026#39;) #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Days with highest total orders:\\n{highest_daily_orders.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Days with lowest total orders:\\n{lowest_daily_orders.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;*use as references to maximize/prioritize advertisement\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #find hourly order and convert to dataframe hourly_order_df = pd.DataFrame(clean_sales_2019_df.groupby(clean_sales_2019_df[\u0026#39;date\u0026#39;].dt.hour)[\u0026#39;order_id\u0026#39;].nunique()) hourly_order_df.columns = [\u0026#39;total_orders\u0026#39;] hourly_order_df.index = pd.to_datetime(hourly_order_df.index, format=\u0026#39;%H\u0026#39;).strftime(\u0026#39;%H:%M\u0026#39;) hourly_order_df.index.name = \u0026#39;hour\u0026#39; hourly_order_df = hourly_order_df.reset_index() #find average hourly order average_hourly_order = hourly_order_df[\u0026#39;total_orders\u0026#39;].mean() #generate hourly order visual with plt.style.context(\u0026#39;bmh\u0026#39;): plt.figure(figsize=(16, 4)) sns.lineplot(x=\u0026#39;hour\u0026#39;, y=\u0026#39;total_orders\u0026#39;, data=hourly_order_df) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Hourly Order Trend\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Hour\u0026#34;) plt.ylabel(\u0026#34;Order\u0026#34;) #add axh line plt.axhline(y=average_hourly_order, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=2) plt.text(0.5, average_hourly_order+100, f\u0026#34;Avg: {average_hourly_order:,.2f}\u0026#34;, color=\u0026#39;red\u0026#39;, fontsize=10, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) #add data callouts for x, y in enumerate(hourly_order_df[\u0026#39;total_orders\u0026#39;]): label = f\u0026#34;{y:,}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#39;offset points\u0026#39;, xytext=(2,4), ha=\u0026#39;center\u0026#39;) plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 #find 6 highest value in hourly orders highest_hourly_orders = hourly_order_df.nlargest(6, \u0026#39;total_orders\u0026#39;) #find 6 lowest value in hourly orders lowest_hourly_orders = hourly_order_df.nsmallest(6, \u0026#39;total_orders\u0026#39;) #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Hours with highest total orders:\\n{highest_hourly_orders.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Hours with lowest total orders:\\n{lowest_hourly_orders.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;*use as references to maximize/prioritize advertisement\u0026#34;) Task 5 Q: What product sold the most? Why do you think it sold the most? A: Find Products Popularity and Product Price List Comparision\nFind data related to Product Popularity by calculate sum of quantity by each product. This return the total product being purchased. It also find price list by product which is used to create filled lines for comparision.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #create dataframe for product popularity product_popularity_df = pd.DataFrame(clean_sales_2019_df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;quantity\u0026#39;].sum()) product_popularity_df.index.name = \u0026#39;product\u0026#39; product_popularity_df = product_popularity_df.sort_values(by=\u0026#39;quantity\u0026#39;, ascending=False) product_popularity_df = product_popularity_df.reset_index() #create dataframe for product price product_price_df = pd.DataFrame(clean_sales_2019_df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;price\u0026#39;].first()) product_price_df = product_price_df.reset_index() #merge the two dataframes on the product column merged_popularity_price_df = pd.merge(product_popularity_df, product_price_df, on=\u0026#39;product\u0026#39;) #generate product sales visual with plt.style.context(\u0026#39;classic\u0026#39;): fig, ax1 = plt.subplots(figsize=(16, 4)) sns.barplot(x=\u0026#39;product\u0026#39;, y=\u0026#39;quantity\u0026#39;, data=merged_popularity_price_df, palette=\u0026#39;husl\u0026#39;, ax=ax1) plt.ticklabel_format(style=\u0026#39;plain\u0026#39;, axis=\u0026#39;y\u0026#39;) plt.title(\u0026#34;Product Popularity\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Product\u0026#34;) plt.ylabel(\u0026#34;Quantity\u0026#34;) plt.xticks(rotation=80) #add data callouts for x, y in enumerate(merged_popularity_price_df[\u0026#39;quantity\u0026#39;]): label = f\u0026#34;{y:,}\u0026#34; plt.annotate(label, (x,y), textcoords=\u0026#34;offset points\u0026#34;, xytext=(0,4), ha=\u0026#39;center\u0026#39;, fontsize=12) #plot product prices as filled line plot ax2 = ax1.twinx() ax2.plot(merged_popularity_price_df[\u0026#39;product\u0026#39;], merged_popularity_price_df[\u0026#39;price\u0026#39;], color=\u0026#39;red\u0026#39;) ax2.set_ylabel(\u0026#34;Price\u0026#34;) ax2.fill_between(merged_popularity_price_df[\u0026#39;product\u0026#39;], merged_popularity_price_df[\u0026#39;price\u0026#39;], alpha=0.2) plt.show() 1 2 3 4 5 6 7 8 9 #create dictionary for product colors for use in product related visual product_colors = {} n_products = len(merged_popularity_price_df[\u0026#39;product\u0026#39;]) palette = sns.color_palette(\u0026#39;husl\u0026#39;, n_colors=n_products) #use same palette as previous product visual for i, product in enumerate(merged_popularity_price_df[\u0026#39;product\u0026#39;]): product_colors[product] = palette[i] #check the state colors print(product_colors) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #find product sold most product_sold_most = merged_popularity_price_df.nlargest(3, \u0026#39;quantity\u0026#39;) product_sold_most_names = product_sold_most[\u0026#39;product\u0026#39;].tolist() product_sold_most_quantities = product_sold_most[\u0026#39;quantity\u0026#39;].tolist() #find product sold least product_sold_least = merged_popularity_price_df.nsmallest(3, \u0026#39;quantity\u0026#39;) product_sold_least_names = product_sold_least[\u0026#39;product\u0026#39;].tolist() product_sold_least_quantities = product_sold_least[\u0026#39;quantity\u0026#39;].tolist() #show result as visual with plt.style.context(\u0026#39;classic\u0026#39;): plt.subplots(figsize=(8, 1)) plt.axis(\u0026#39;off\u0026#39;) plt.text(0.5, 0.5, f\u0026#34;Most sold: {product_sold_most_names[0]}, Quantity: {product_sold_most_quantities[0]:,}\\n\u0026#34; f\u0026#34;Least sold: {product_sold_least_names[0]}, Quantity: {product_sold_least_quantities[0]:,}\u0026#34;, fontsize=14, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) plt.title(\u0026#34;Most \u0026amp; Least Sold Product\u0026#34;, fontsize=20) plt.show() 1 2 3 4 5 #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Top 3 most sold products:\\n{product_sold_most.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Bottom 3 least sold products:\\n{product_sold_least.to_string(index=False)}\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #find price for each item list product_list_df = pd.DataFrame(clean_sales_2019_df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;price\u0026#39;].first()) product_list_df = product_list_df.sort_values(by=\u0026#39;price\u0026#39;, ascending=False) product_list_df = product_list_df.reset_index() #find product price average average_product_price = product_list_df[\u0026#39;price\u0026#39;].mean() #generate product list visual with plt.style.context(\u0026#39;classic\u0026#39;): plt.figure(figsize=(16, 6)) sns.barplot(x=\u0026#39;price\u0026#39;, y=\u0026#39;product\u0026#39;, data=product_list_df, palette=product_colors) plt.title(\u0026#34;Product Price List\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Price\u0026#34;) plt.ylabel(\u0026#34;Product\u0026#34;) #add axv line plt.axvline(x=average_product_price, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=1) plt.text(average_product_price+5, len(product_list_df)/2, f\u0026#34;Avg: ${average_product_price:.2f}\u0026#34;, color=\u0026#39;red\u0026#39;, ha=\u0026#39;left\u0026#39;, va=\u0026#39;center\u0026#39;) plt.show() 1 2 3 4 5 6 7 8 9 10 11 #find expensive product most_expensive_product = product_list_df.nlargest(3, \u0026#39;price\u0026#39;) #find cheap product most_cheap_product = product_list_df.nsmallest(3, \u0026#39;price\u0026#39;) #print result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;Most expensive products:\\n{most_expensive_product.to_string(index=False)}\\n\u0026#34; f\u0026#34;\\n\u0026#34; f\u0026#34;Most cheap products:\\n{most_cheap_product.to_string(index=False)}\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 #find average quantity per order id average_item_per_order = clean_sales_2019_df.groupby(\u0026#39;order_id\u0026#39;)[\u0026#39;quantity\u0026#39;].mean().mean() #show result as visual with plt.style.context(\u0026#39;classic\u0026#39;): plt.subplots(figsize=(8, 1)) plt.axis(\u0026#39;off\u0026#39;) plt.text(0.5, 0.5, f\u0026#34;{average_item_per_order:,.2f}\u0026#34;, fontsize=40, ha=\u0026#39;center\u0026#39;, va=\u0026#39;center\u0026#39;) plt.title(\u0026#34;Average Item Each Order\u0026#34;, fontsize=20) plt.show() Task 6 Q: How much probability for next people will order certain products? A: Find Product Orders % of Total Sales\nThe following section is to find % product orders/sold compare to the total sales of all products. the result will give % of product which may define the likelihood of the product will be purchased next. This part is also the only section which used Squarify to plot the result as Treemap.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #find total product sold total_product_sold = product_popularity_df[\u0026#39;quantity\u0026#39;].sum() #find sizes and labels value in product popularity sizes = product_popularity_df[\u0026#39;quantity\u0026#39;] labels = product_popularity_df[\u0026#39;product\u0026#39;] #calculate percentage for each product percentages = [f\u0026#39;{100*sizes[i]/sizes.sum():.2f}%\\n{labels[i]}\u0026#39; for i in range(len(sizes))] #generate treemap visual for product sold percentage of total with plt.style.context(\u0026#39;seaborn-dark-palette\u0026#39;): plt.figure(figsize=(16, 8)) squarify.plot(sizes=sizes, label=percentages, alpha=0.7, color=[product_colors[label] for label in labels]) #get color from previous product color dict plt.axis(\u0026#39;off\u0026#39;) plt.title(\u0026#34;Product % of Total\u0026#34;, fontsize=20) plt.show() 1 2 3 4 5 6 7 8 #print the result as conclusion print(f\u0026#34;CONCLUSION\\n\u0026#34; f\u0026#34;List of probability:\u0026#34;) #find percentages for product and print as conclusion for p in percentages: prob, label = p.split(\u0026#39;\\n\u0026#39;) print(f\u0026#34;{label} = {prob}\u0026#34;) Task 7 Q: What products are most often sold together? A: Find Product Associate Rules using mlxtend library\nFirst need to find the product sold by each order id and store them into series(product_by_order). Then transform the data into one-hot encoded matrix by applying TransactionEncoder().fit_transform(). The result will be stored as dataframe(encoded_df). Encoded data then used to generate frequent item sets using apriori algorithm the minimum support are defined as following. Lastly sort the order by value in \u0026lsquo;confidence\u0026rsquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #group product by order id product_by_order = clean_sales_2019_df.groupby(\u0026#39;order_id\u0026#39;)[\u0026#39;product\u0026#39;].apply(list) #convert data to one-hot encoded matrix te = TransactionEncoder() onehot = te.fit_transform(product_by_order) encoded_df = pd.DataFrame(onehot, columns=te.columns_) #identify frequent itemsets frequent_itemsets = apriori(encoded_df, min_support=0.000015, use_colnames=True) #generate association rules rules = association_rules(frequent_itemsets, metric=\u0026#39;confidence\u0026#39;, min_threshold=0.5) rules = rules.sort_values(\u0026#39;confidence\u0026#39;, ascending=False) #print rules data head rules.head() Although the rules can be presented as tables. In this case the rules is generated as string following more descriptive explanation. There may be other method to visualize the rules waiting to be explored. But for now the following will do.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #create summary for each rule #create list to store items summaries = [] #iterrate throught available items in rules for i, row in rules.iterrows(): antecedents = \u0026#39;, \u0026#39;.join(list(row[\u0026#39;antecedents\u0026#39;])) consequents = list(row[\u0026#39;consequents\u0026#39;])[0] support = row[\u0026#39;support\u0026#39;] confidence = row[\u0026#39;confidence\u0026#39;] lift = row[\u0026#39;lift\u0026#39;] leverage = row[\u0026#39;leverage\u0026#39;] conviction = row[\u0026#39;conviction\u0026#39;] #define ANSI escape codes for modifying the text\u0026#39;s appearance BLUE = \u0026#39;\\033[94m\u0026#39; GREEN = \u0026#39;\\033[92m\u0026#39; RED = \u0026#34;\\033[91m\u0026#34; END = \u0026#39;\\033[0m\u0026#39; #create summary in string summary = ( f\u0026#34;Rule {i+1}:\\n\u0026#34; f\u0026#34;Customers who bought {BLUE}{antecedents}{END} are more likely to buy {RED}{consequents}{END}.\\n\u0026#34; f\u0026#34;This combination of products was purchased in {GREEN}{support*100:.3f}%{END} of all transactions.\\n\u0026#34; f\u0026#34;When customers buy {BLUE}{antecedents}{END}, the chance of them also buying {RED}{consequents}{END} is {GREEN}{confidence*100:.2f}%{END}.\\n\u0026#34; f\u0026#34;{RED}{consequents}{END} appears {GREEN}{lift:.2f}x{END} more likely when {BLUE}{antecedents}{END} appears compared to {RED}{consequents}{END} alone in general.\\n\u0026#34; f\u0026#34;The rule\u0026#39;s significance, as measured by leverage, is {GREEN}{leverage:.5f}{END}.\\n\u0026#34; f\u0026#34;Dependence between {BLUE}{antecedents}{END} and {RED}{consequents}{END} is {GREEN}{conviction:.2f}{END}.\u0026#34; ) #append the summary summaries.append(summary) #join all summaries into a single string summary_string = \u0026#34;\\n\\n\u0026#34;.join(summaries) #print summary print(f\u0026#34;CONCLUSION\\n\\n{summary_string}\u0026#34;) Summarize It appears that the company generated a total of USD 34,465,537.94 in sales, with the majority of sales occurring in 2019, totaling USD 34,456,867.65. In contrast, 2020 only accounted for USD 8,670.29 in sales, which may be due to incomplete or missing data. The highest selling months were December, October, and April 2019, with sales of USD 4,608,295.70, USD 3,734,777.86 and USD 3,389,217.98 respectively, while the lowest sales occurred in January, September 2019, and January 2020, which accounted for only USD 3,915,878.85 of the total sales. The company generated the highest sales in California (CA), with San Francisco and Los Angeles accounting for USD 8,254,743.55 and USD 5,448,304.28 respectively. The lowest sales occurred in Portland, Maine, with only USD 449,321.38 in sales.\nTo maximize the company\u0026rsquo;s advertising efforts, it is recommended to advertise in December, October, and April, as the company generated the most orders during these months (24,004, 19,436, and 17,528, respectively). Tuesdays were found to be the most profitable day for advertising, generating 26,063 orders, while Fridays had the lowest number of orders. The best times for advertising were between 11:00-13:00 and 18:00-20:00, when the company typically had the highest sales counts.\nIn terms of product performance, some products performed better or worse than their price indicated. The finding result is as following:\nThe Bose SoundSport Headphones had a lower price, but sold less than the Apple Airpods Headphones. The 34in Ultrawide Monitor, 27in 4K Gaming Monitor, Flatscreen TV, 20in Monitor, Vareebadd Phone, LG Washing Machine, and LG Dryer all sold fewer units than the iPhone despite having lower prices. In contrast, the ThinkPad Laptop and Macbook Pro Laptop performed better than expected when compared to the LG Dryer, LG Washing Machine, Vareebadd Phone, and 20in Monitor despite having higher price. The 20in Monitor was the worst-performing product, selling only 4,123 units, compared to the 27in FHD Monitor, which sold 7,538 units despite having lower price. Finally, the probability of customers ordering specific products compared to all other products was calculated as following:\nThe USB-C Charging Cable had the highest probability, with 11.46%. Followed by the Wired Headphones at 9.83%. The iPhone had a probability of 3.28%. While the Google Phone had a probability of 2.65%. Additionally, when customers bought Apple Airpods Headphones, Wired Headphones, and Lightning Charging Cable, the chance of them also buying an iPhone was 100%, but this accounted for only 0.002% of total transactions. Similarly, when customers bought Wired Headphones, USB-C Charging Cable, and Bose SoundSport Headphones, the chance of them also buying a Google Phone was 60%, but this accounted for only 0.002% of total transactions.\nDataset\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://httplife.com/projects/data-analysis/","summary":"Objective The purpose is to provide analysis based on dataset1 provided by going through all the necessary step and methods in this alongside explanation. My project collects data of sales records. I have wrangled and analysed data from last couple years of each country individually and visualized the data in form of excel dashboard for easy understanding of lending trend.\nImport \u0026amp;\u0026amp; Read Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import os import pandas as pd import matplotlib.","title":"Data Analysis"},{"content":"The Idea Raising awareness about anti-Asian hate crimes and tracking incidents can help shine a light on the issue and potentially lead to solutions or interventions.\nIt need to collect and aggregate the data with best effort. And it collect anti-Asian incidents from online public sources and provide hyperlinks (accompanied by titles and brief excerpts of the reports) that direct you to the original sources of information.\nThe Challenges There are several challenges to creating a hate crime tracking website, including:\nData collection Collecting reliable and accurate data can be challenging, particularly when it comes to hate crimes. Not all incidents may be reported or documented, and some may be misreported or misclassified. Data privacy Collecting and sharing sensitive data related to hate crimes can raise privacy concerns for individuals and communities affected by these incidents. It\u0026rsquo;s important to ensure that proper safeguards are in place to protect the privacy and confidentiality of those involved. Resources Building and maintaining a website to track hate crimes requires resources, including funding, personnel, and technical expertise. It can be challenging to secure these resources, particularly for grassroots or community-based initiatives. Legal considerations The legal landscape around hate crimes can be complex, and it\u0026rsquo;s important to ensure that any website or data collection effort is in compliance with relevant laws and regulations. Impact Sharing information about hate crimes can have a psychological impact on individuals and communities affected by these incidents. It\u0026rsquo;s important to consider the potential impact of the website on these groups and to provide resources and support as needed. The Design 1 When designing the website and portal, I think it\u0026rsquo;s important to consider the needs of users and ensure that the interface is intuitive and easy to use. Therefore, I have designed a website that includes data visualization with interaction features such as charts, maps, and tables to provide a clear and direct view of ongoing anti-Asian hate crime incidents on a daily basis. Additionally, I have created a portal that allows members to log in and manage news sources, ensuring that the data collection process is efficient and effective. Lastly, the website also includes a comment feature to facilitate discussion and engagement around the issue.\nThe Code Gather police departmetn twitter ids manually and store it as array, fetch tweets by id using python, save result to database[PostgreSQL]\n1 2 3 4 5 6 7 8 9 10 11 12 for twitter_id in twitter_ids: # ... configuration tweets = twint.output.tweets_list for tweet in tweets: data = ( tweet.timestamp, tweet.user_id, tweet.username, tweet.name, tweet.tweet, tweet.mentions, tweet.urls, tweet.hashtags, tweet.cashtags, tweet.link, tweet.geo, tweet.source ) try: insert_data(data) except (Exception, psycopg2.DatabaseError) as error: print(error) When we gather enough data from twitter with all categories that we needs, sort it by hashtags and use NLP to analysis each tweets whether it contains hate crimes informations.\nFrontend 2 Taking advantage of ReactJS funcitonal componment, It allows me to create Context to fetch JSON data from Backend API and present it.\nBackend 3 Using the Twitter API, it is possible to fetch data that is related to specific keywords or topics from the Twitter platform. This allowing me to access and analyze relevant content, which can be valuable for a variety of purposes, such as tracking hate crime in specific area and date range. Furthermore, We utilized Python Flask to develop lightweight API and share it to public.\nFigma design file\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGithub repository\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nData mining source code\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://httplife.com/projects/hate-crime-tracker/","summary":"The Idea Raising awareness about anti-Asian hate crimes and tracking incidents can help shine a light on the issue and potentially lead to solutions or interventions.\nIt need to collect and aggregate the data with best effort. And it collect anti-Asian incidents from online public sources and provide hyperlinks (accompanied by titles and brief excerpts of the reports) that direct you to the original sources of information.\nThe Challenges There are several challenges to creating a hate crime tracking website, including:","title":"Hate Crime Tracker"}]